1. In the A/B test analysis, I believe that we are p-hacking. I don't think coming to a conclusion at p<0.05 would be comfortable enough for the dataset.
We see that the might not be much difference in the 'all users' case, but for instructions only, the p-values are borderline significant at 0.05, suggesting a weak indication that the instructors may have searched more with the new design.
2. If we have done a T-test between each pair of sorting implementations, the number of tests we need to perform is 21 ( 7 chooses 2 ).
Each individual T-test typically uses a significance level of 0.05%. With multiple tests, the probability of at least one false positive increases
P(false positive) = 1 - ( 1-0.05)^21 = 0.667(66.7%)
Bonferroni correction adjusts for multiple comparisons b dividing the significance level by the number of tests: 0.05/21 = 0.00238
3. Sorting Implementations and Their Mean Execution Time:
        algorithm  time_taken
5             qs4    0.042802
4             qs3    0.042360
0          merge1    0.042146
3             qs2    0.041542
6             qs5    0.038692
2             qs1    0.029882
1  partition_sort    0.023206

Sorting Implementations Ranked by Speed (Lower is Faster):
        algorithm  rank
5             qs4   7.0
4             qs3   6.0
0          merge1   5.0
3             qs2   4.0
6             qs5   3.0
2             qs1   2.0
1  partition_sort   1.0

ANOVA Results:
F-statistic = 83.173, p-value = 0.00000000

Pairwise Tukey's HSD Test Results:
        Multiple Comparison of Means - Tukey HSD, FWER=0.50
====================================================================
    group1         group2     meandiff p-adj   lower   upper  reject
--------------------------------------------------------------------
        merge1 partition_sort  -0.0189    0.0 -0.0212 -0.0167   True
        merge1            qs1  -0.0123    0.0 -0.0145   -0.01   True
        merge1            qs2  -0.0006 0.9987 -0.0028  0.0016  False
        merge1            qs3   0.0002    1.0  -0.002  0.0024  False
        merge1            qs4   0.0007 0.9979 -0.0016  0.0029  False
        merge1            qs5  -0.0035  0.066 -0.0057 -0.0012   True
partition_sort            qs1   0.0067    0.0  0.0044  0.0089   True
partition_sort            qs2   0.0183    0.0  0.0161  0.0206   True
partition_sort            qs3   0.0192    0.0  0.0169  0.0214   True
partition_sort            qs4   0.0196    0.0  0.0174  0.0218   True
partition_sort            qs5   0.0155    0.0  0.0133  0.0177   True
           qs1            qs2   0.0117    0.0  0.0094  0.0139   True
           qs1            qs3   0.0125    0.0  0.0102  0.0147   True
           qs1            qs4   0.0129    0.0  0.0107  0.0152   True
           qs1            qs5   0.0088    0.0  0.0066   0.011   True
           qs2            qs3   0.0008  0.993 -0.0014  0.0031  False
           qs2            qs4   0.0013 0.9382  -0.001  0.0035  False
           qs2            qs5  -0.0028 0.2113 -0.0051 -0.0006   True
           qs3            qs4   0.0004 0.9998 -0.0018  0.0027  False
           qs3            qs5  -0.0037  0.041 -0.0059 -0.0014   True
           qs4            qs5  -0.0041 0.0138 -0.0063 -0.0019   True
--------------------------------------------------------------------
Indistinguishable Implements are pairs where they did not show statistically differences, or in another words, where reject=False.
merge1, qs2, qs3, qs4 perform similarly and are indistinguishable. They are among the slowest running time algorithms
Partition_sort is the fastest with the lowest running time. 
